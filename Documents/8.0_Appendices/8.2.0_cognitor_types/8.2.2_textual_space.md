# Textual Space 中的 Cognitors 属性

> 基于 [[8.2.1_introduction]] [[1.1.1_protocol_requirements]]

ACP Textual Space 在当前章节中简称为 Space。

> TODO: 可能需要区分人类的意识-身体，计算机硬件-encoder/decoder-LLM-LLM Agent

## 1. 人类 (Human)

### 1.1 定义

指生物学意义上的自然人，且拥有内在的、完整的自然认知能力体系。

### 1.2 核心特性

#### 1.2.1 原生认知

天然具备学习、推理、元认知、创造力、常识、情感、意图和社会智能。

#### 1.2.2 原生记忆

具有连续、跨场景的**持久性记忆系统**，包括：

- **跨Space记忆保留**：可自然保持对多个独立`Space`交互历史的记忆。
- **经验累积**：能主动关联不同`Space`中的知识片段形成认知网络。
- **元记忆能力**：对自身记忆的完整性、准确度具有评估能力。

#### 1.2.3 具身能力 (Embodied Capability)

- **物理交互能力**: 可直接操控现实世界（如操作设备、调整环境、执行实验），而不仅限于符号层面的计算。
- **多模态感知**: 通过视觉、听觉、触觉、嗅觉等感官实时获取信息，并动态调整认知策略。
- **即时反馈调整**: 能在现实交互中实时修正行为（如调试代码、调整实验参数），而非仅依赖预定义的认知轨迹回溯。
- **主观能动性**: 可自主决定何时、如何介入 ACP 交互（如主动退出 Space、修改上下文、引入外部数据）。
- **跨媒介执行**: 能同时在 `Space` 和物理世界（实验室、社交环境）中协调任务。

#### 1.2.4 深度情境理解

擅长理解隐含意义、语用和复杂上下文。

#### 1.2.5 主观性与能动性

拥有意识、主观体验和内在驱动力。

#### 1.2.6 具身与社会性

认知受物理身体和社会环境影响。

### 1.3 局限性

人类作为 Cognitor 在 Space 中的局限性包括：

#### 1.3.1 认知局限

- 受限于**注意力**、当前**知识范围**和**信息处理速度**，难以处理海量或高复杂度信息。
- 可能受**偏见**、**情绪**和**先有经验**影响，导致判断失误或视野狭窄。
- 容易受到**外部干扰**或自身状态（如疲劳、压力）影响而**分心**，导致任务中断或偏离。
- 对许多使用的符号（语言、数学符号、抽象概念如“正义”、“自由”、“爱”等）的理解，并非直接对应于可触摸或看到的具体物体，而是通过文化共识、定义、经验的关联以及在特定语境中的使用来建立的。不同文化、不同个体对同一抽象符号的理解可能存在差异。
- 对世界的学习和理解，很大一部分是通过语言、书籍、图像、视频等符号系统传递的间接经验。理解的准确性和深度取决于符号系统的质量、传递者的意图以及接收者的解读能力。如果符号系统存在偏差或信息不完整，理解也会出现偏差。
- 试图用语言（符号）来表达内在的主观体验，例如感受、情绪、意识等等，但语言本身是有限的，难以完全捕捉和传递这些主观体验的丰富性和复杂性。个体描述感受可能与他人理解的不同，因为个人经历和感受不同。
- 由于不同的文化和语言使用不同的符号系统来指代世界和概念，一个符号在一个文化中可能具有明确的意义，但在另一个文化中可能完全不同或不存在对应的概念。
- 记忆和认知过程并非完美，会受到各种偏差的影响。对过去的事件的符号化回忆可能会失真或被重构。对世界的理解也会受到先验知识、信念和偏见的影响，导致对新信息的解读产生偏差。

#### 1.3.2 精力与时间限制

- 需要**休息**，无法进行不间断、高强度的持续工作，容易**疲劳**。
- 同一时间只能处理有限的任务，**多任务处理能力有限**。

#### 1.3.3 行为惰性与非理性

- 可能表现出**懒惰**、**拖延**，规避复杂或重复性任务。
- 可能因**厌烦**、**挫败感**等情绪而放弃或**抵触**交互。
- 可能过度依赖自动化或AI，**减少自主思考或验证**。

#### 1.3.4 易出错性

- 在输入、逻辑推理、数据解读或操作过程中容易**犯错**。
- **记忆不完美**，会随时间衰退，且可能遗忘或混淆细节。

### 1.4 补偿机制 (Compensation Mechanisms)

人类作为 Cognitor 在 ACP Textual Space 中的交互，可以通过多种机制来补偿自身的固有局限性：

#### 1.4.1 Leveraging Other Cognitors and Tools (利用其他认知实体与工具)

- **委托任务**: 人类可以将需要处理海量信息、高强度计算、重复性操作或超出自身当前知识范围的任务，通过认知指令 (`Cognitive Directive`, CD) 委托给 LLM Agent 或其他自动化 Cognitor 执行。这补偿了人类在**认知局限**（信息处理速度、知识范围）、**精力与时间限制**以及**行为惰性**方面的不足。
- **信息整合与提炼**: 利用 LLM Agent 快速整合和提炼 Space 中的海量历史符号流或外部信息源，为人类提供精简、摘要或结构化的信息，从而帮助人类克服**注意力**和**信息处理速度**的局限。
- **利用工具能力**: 通过 ACP 协议允许的机制（例如通过向特定的 Agent 发送带有工具使用指令的 CD），人类可以利用外部工具（如搜索引擎、数据库、编程环境）来获取实时信息、执行复杂计算或物理操作，从而补偿**知识截止**、**符号接地问题**（通过获取现实世界的反馈）和**领域专精度不足**的局限。

#### 1.4.2 Utilizing the Common Space as Persistent Memory and Shared Context (利用认知空间作为持久记忆和共享上下文)

- **外部记忆存储**: Common Space 中包含所有带有归因的、按时间/因果组织的**外显符号 (`Externalized Sign`)**流（包括常规交互和强制性的 `CT Sign`），这构成了 ACP 交互的持久化记录。人类可以随时回溯和访问这些历史记录，以此补偿自身的**记忆不完美**、**遗忘**以及**会话隔离性**的局限。
- **共享理解锚点**: Space 中的历史交互和共识符号为多实体协作提供了一个共享的上下文和理解基础，减少因个体记忆差异或理解偏差导致的协作问题。这辅助了人类对**深层情境**和**抽象符号**的理解。

#### 1.4.3 Focusing on Higher-Order Cognitive Functions (专注于更高阶认知功能)

- **战略决策与判断**: 将低层级、重复性的符号处理委托给 Agent 后，人类可以专注于利用其**原生认知**中的推理、创造力、常识、情感和深度情境理解能力，进行高层级的战略规划、复杂问题的判断和解决、以及涉及价值观和伦理的决策。这发挥了人类作为“**最终解释者**”和**主观能动性**的作用。
- **元认知与反思**: 利用 ACP 强制要求的 CT 系统记录的认知过程踪迹（包括 Agent 和自身的），人类可以进行更深入的元认知反思，审计自身的思考过程、识别偏见或逻辑错误，并理解其他 Cognitor 的行为模式。这直接补偿了人类自身**认知局限**中的**偏见**和**易出错性**，并促进了跨实体理解和调试。

#### 1.4.4 Applying Constraints and Directives (CDs) for Guidance and Control (应用约束和指令进行引导与控制)

- **结构化输入与约束**: 人类可以通过精心设计和发出带有明确约束的 `Cognitive Directive` (CD)符号，来精细控制 Agent 的行为和符号过程方向，减少其**幻觉**、**不可控输出**和**文化偏差**的可能性。CDs 提供了将人类意图和外部知识转化为 Agent 可处理的**约束**的机制。
- **修正与引导**: 当发现 Agent 的行为或输出偏离预期时，人类可以立即通过新的 CD 或直接修改 Space 中的上下文（如编辑 CT 节点，如果协议允许且人类具备权限），来修正 Agent 的认知过程和后续行动。这利用了人类作为**协议弹性维护者**和**物理世界锚点**的身份。

#### 1.4.5 Physical World Interaction and Data Injection (物理世界交互与数据注入)

- **符号接地**: 人类可以通过其**具身能力**直接感知和交互物理世界，将抽象符号与现实世界实体或事件关联起来，并将这些真实的观察结果作为新的带有明确归因的符号注入到 Space 中，从而辅助所有 Cognitor 解决**符号接地问题**。
- **引入外部知识**: 人类可以从 ACP Space 之外的各种来源（如书籍、互联网、现实交流、实验结果）获取信息，并将其以外显符号的形式带入 Space，补偿其他 Cognitor（尤其是 LLM Agent）可能存在的**知识截止**和**领域专精度不足**。

#### 1.4.6 Collaboration and Negotiation (协作与协商)

- **跨实体校准**: 通过与 Space 中的其他人类或 Agent 进行符号交换，进行讨论、提问、解释和协商，可以澄清模糊的符号含义、解决分歧、达成共识，共同确定符号的 `Object`，从而克服个体在**抽象符号理解**和**文化/语言偏差**上的局限。
- **互补视角**: 不同的 Cognitor 拥有不同的知识、能力和视角。通过协作，人类可以利用这些互补性来更全面地理解问题和构建意义，补偿自身的**视野狭窄**和**认知局限**。

### 1.5 在 ACP 中的角色

* 典型的交互**发起者、指令者、评估者和最终解释者**。
* 可以作为**协作者**与其他 `Cognitor` 合作。
* 可以**扮演角色**，即作为 `PersonaCognitor` 的扮演者。
* 通常拥有对 ACP 环境的元认知理解和控制权（如开发者）。
* **物理世界锚点**: 通常，人类是 ACP Textual Space 与真实世界的**唯一天然桥梁**，例如：
    * 将实验数据手动录入 Space。
    * 根据现实反馈修正 `Object` 的约束条件。
    * 真实执行代码
* **协议弹性维护者**: 当 LLM Agent 陷入逻辑循环或知识盲区时，人类可通过具身行动打破僵局（如实地调查补充数据）。
* 有时可以**修改上下文**，例如，人类可以直接修改已有的 `Canvas` 中的某个 `ct` 节点，来控制 `LLM Agent` 的行为。

### 1.6 认知轨迹与透明性

* 核心认知过程内在。在标准交互中通常不强制要求详细的外显认知轨迹，其元认知通过行为、反馈或显式沟通体现。协议不排除在特定高透明度协作场景下要求人类记录决策过程的可能性。

## 2. LLM (大型语言模型)
#需澄清
### 2.1 定义

指通过在海量文本和数据上进行训练而形成的一种**大规模的统计语言模型**。其核心机制在于学习和捕捉训练数据中的语言结构、事实知识、模式和关联，并能够基于给定的输入（Prompt）**预测序列中的下一个词（Token）**，从而生成连贯、语法正确的文本序列。

LLM 本身是一个**无意识、无主观体验、无内在驱动力**的**概率计算引擎**。它不直接感知现实世界或拥有物理具身性。在 ACP 框架中，原始 LLM 被视为驱动 `LLM Agent` 和某些 `PersonaCognitor` 的**底层技术基础或认知基底**。它本身不直接作为独立的 `Cognitor` 参与 Space 中的交互。

**类比**：可以将原始 LLM 理解为一块拥有巨大计算和模式匹配能力的“芯片”或“大脑硬件”，它需要通过外部的“操作系统”和“应用程序”（例如 `LLM Agent` 的实现逻辑、ACP 协议、工具调用能力等）才能在 `Space` 中执行有意义的任务并与环境互动。

### 2.2 核心特性

- **统计模式学习**: 能够从海量训练数据中学习并复制复杂的语言模式、语法结构、写作风格和潜在的信息关联。
- **生成能力**: 能够基于输入 Prompt 生成新的文本内容，其质量和相关性取决于模型规模、训练数据和输入质量。
- **参数化知识**: 通过数以亿计甚至万亿计的模型参数隐式地存储了训练数据中蕴含的知识和信息。
- **概率性输出**: 生成的文本基于对不同 Token 出现概率的计算，通常包含一定的随机性（可通过采样参数控制），使得输出具有多样性。
- **非持久状态 (单次调用)**: 原始 LLM 模型本身通常是无状态的，对单次 API 调用或推理过程不保留跨越不同调用或 Prompt 的内部记忆或上下文（跨调用记忆由外部系统或 `LLM Agent` 层面维护）。

### 2.3 局限性

- **知识截止**: 其知识内容仅限于训练数据的收集时间点，无法获取或反映训练完成后发生的新事件、最新研究进展或实时信息。
- **缺乏地面真值验证**: 模型本身无法独立验证其生成的信息是否与现实世界的客观事实一致。其输出是基于训练数据中的模式和概率，这可能导致生成看似合理但虚构或错误的内容（即“幻觉”现象的根本原因之一）。
- **与人类不相同的理解或意识**: 尽管能够处理和生成复杂的语言，但 LLM 的语义理解、自我意识、主观体验或推理能力与人类极为不同，其行为是基于统计关联而非逻辑推理或世界模型。
- **无具身性**: 无法直接感知物理世界、进行现实世界操作或从物理交互中获取反馈，其与外部世界的连接完全依赖于输入的符号化信息。
- **训练数据引入的偏差**: 继承了训练数据中存在的各种偏见（文化、社会、历史等），可能在输出中体现出来。
- **知识固化**: 其核心能力和知识在训练完成后相对固定，无法在运行时动态地、永久地学习全新知识或技能（不经过微调或额外的训练）。

### 2.4 补偿机制

- 原始 LLM 作为底层模型，不具备自身的补偿机制。对其局限性的补偿通常通过构建在 LLM 之上的系统或代理层（如 `LLM Agent` 利用工具、外部知识库、特定的推理框架或人类反馈）来实现。

### 2.5 在 ACP 中的角色

- 作为 **`LLM Agent` 的核心计算引擎和语言能力提供者**。
- 提供语言理解和生成的基础能力，支撑 `LLM Agent` 在 Space 中进行符号处理和交互。
- 可以作为实现特定 **`PersonaCognitor`** 的技术载体，为其赋予文本生成和理解能力。

### 2.6 认知轨迹与透明性

- 原始 LLM 的内部工作（例如神经网络的计算、参数激活）是高度复杂且通常被视为**黑箱**，不产生人类或 `LLM Agent` 意义上的、可外部观察和理解的“认知轨迹”。
- ACP 框架中关于认知轨迹和透明性的要求，是针对在 Space 中实际进行交互的 **`Cognitor` 实体**（特别是 `LLM Agent`），而非底层原始 LLM 模型本身。

---

## 3. PersonaCognitor (角色认知体)
### 3.1 定义

* 指被**显式设计或主动扮演或模拟**成具有**特定、明确的角色、个性或人物形象**的认知实体。其核心在于**优先模拟或展现该特定身份**的行为模式、语言风格、知识范围和可能的互动倾向。其认知对扮演者完全透明。

### 3.3 核心特性

* **角色中心**: 行为高度受角色设定约束。
* **个性化与风格化**: 展现鲜明、一致的个性、语言风格或情感倾向。
* **特定知识/视角模拟**: 可能模拟特定专家、虚构人物、或代表某种特定立场/功能。
* **实现载体多样**: 通常由 `LLM` 模拟，也可以由**人类**扮演。
* **交互引导**: 常用于引导特定类型的交互（如教学、娱乐、限定特定领域知识）。

### 3.4 在 ACP 中的角色

* **定制化的交互界面/伙伴**: 提供更自然、更有吸引力或更符合场景需求的互动。
* **模拟与训练环境中的角色**: 如扮演特定 NPC、对手或导师。扮演编程语言环境。
* **特定功能代理**: 如项目协调员、领域专家助手等拟人化代理。
* **视角转换**: 如扮演不同语言的使用者来使用不同的思考方式。

### 3.5 认知轨迹与透明性

* 取决于实现载体。LLM 驱动的可能记录维持角色的过程；人类扮演的遵循人类认知轨迹习惯。`CognitorInfo` 中应清晰描述其扮演的角色、核心个性及实现载体（AI/人类）。

### 3.6 扮演者唯一性

* 在同一个时刻，一个 `PersonaCognitor` 只能由一个 `Cognitor` 扮演。切换扮演者需要在Space中显式声明。

## 4. LLM Agent (大型语言模型代理)

### 4.1 定义

* 指主要由大型语言模型（LLM）驱动，并展现出**经LLM开发者深度定制为通用目的、通常被设定为中立或助手形象、默认状态、自称为 LLM 本身**的 **PersonaCognitor**。
* **注意**：尽管其通常自称为 LLM 本身，但 LLM Agent 在 ACP 中不被视为 LLM 本身。

### 4.2 核心特性

* **数据驱动与通用性**: 其知识和行为主要反映底层 LLM 的训练数据和通用能力。
* **语言中心**: 高度擅长自然语言理解与创建。
* **直接访问模型能力**: 通常被视为直接调用底层 LLM 通用能力的接口。
* **有限的角色扮演**: 默认不被要求严格维持一个狭义的、高度个性化的角色，其"个性"通常是通用、中立或助手式的。通常作为一种“搜索引擎”，仅为了回答问题。

### 4.3 局限性

#### 4.3.1 固有限制

- **知识截止**：由于模型基于特定时间点的数据进行训练，因此对于该时间点之后的信息和事件缺乏了解。
- **幻觉**：指LLM生成的内容可能包含不准确、虚构或者与现实不符的信息。这通常发生在处理复杂问题或超出其训练数据范围的话题时。
- **符号接地问题**：LLM 无法感知到名词的**对象(Object)**。这意味着它们可能无法正确地将语言中的概念与真实世界的现象对应起来。
- **逻辑推理限制**：尽管LLM可以执行一定程度的逻辑推理任务，但当涉及深层次或多步骤的逻辑分析时，可能会出现错误或不一致的结果。
- **文化偏差**：由于训练数据集的局限性，LLM可能会体现出一定的文化偏见，对某些群体或文化的表现可能存在刻板印象或误解。
- **领域专精度不足**：在一些专业领域内，如医学、法律等，由于相关术语和知识的专业性较强，LLM可能无法提供足够准确的回答或建议。
- **不可控输出**：虽然可以通过调整输入（如提示设计）来引导LLM的行为，但是其输出仍然具有一定的随机性和不确定性，尤其是在开放式的对话环境中。
- **伦理与安全挑战**：存在潜在风险，包括但不限于生成有害内容、隐私泄露以及难以确保对话内容符合社会伦理标准等问题。

#### 4.3.2 记忆局限性

- **会话隔离性**：每个`Space`实例形成独立的记忆边界，无法直接跨`Space`传递信息。
- **临时记忆**：仅在当前交互会话中保持上下文记忆（受限于token窗口）。
- **无持久状态**：不同`Space`中的LLM实例互不知晓彼此的存在与活动。
- **知识固化**：依赖预训练参数存储的静态知识，难以自主更新长期记忆。

### 4.4 补偿机制

- 必须通过CT系统完整记录当前`Space`内的认知过程。
- 依赖人类或其他拥有所需能力的`Cognitor`主动进行跨`Space`的知识同步。
- 可通过人类来实际运行代码或其它现实交互。
- 可能可通过 “Tool Use / Function Calling” 来实际运行代码或其它现实交互。
- 可通过跨认知实体执行委托或工具调用（如果有）来缓解**知识截止**和**会话隔离性**。
- 需通过CT系统来增加识别**幻觉**的可能性。
- 通过更大的模型和更多的数据可以一定程度缓解**符号接地问题**。
- 需通过CT系统来进行深度**逻辑推理**。
- 需通过CT系统来识别**文化偏差**。
- 越先进的模型的领域专精度能力越高，可以缓解**领域专精度不足**。
- 需精细设定CT系统的结构，并设定期望输出来缓解**不可控输出**。
- 对**伦理与安全挑战**的应对取决于对风险的定义和接受程度。

### 4.5 在 ACP 中的角色

* 通用的信息提供者和任务执行者。

### 4.6 认知轨迹与透明性

- LLM Agent 的**核心认知过程**（即底层模型的计算、权重激活等）是**内在**且**黑箱化**的。
- 然而，为了满足 ACP 框架对透明性、可追溯性和协作的需求，在标准交互流程中，**强制要求** LLM Agent **外显地输出**其**详细的认知轨迹**。
- 这里的“外显认知轨迹”并**不是**指底层的神经网络计算细节，而是指 LLM Agent 根据其接收到的输入和内部状态，**生成并呈现**的、能够体现其**推理步骤、中间思考过程、信息处理逻辑或决策路径**的文本或结构化数据。例如，“链式思考 (Chain-of-Thought)”、“scratchpad” 中的中间步骤、分解任务的过程等。这需要设计CT系统的结构。
- LLM Agent 的“元认知”（对自身思考过程或状态的模拟性评估）主要通过这种外显的认知轨迹、其产生的行为结果、提供的反馈信息以及其显式的沟通过程本身来体现。
- 这项强制要求确保了 LLM Agent 的工作过程对 ACP 系统中的其他参与者（特别是人类）是**可见且可理解**的，便于审查、调试、纠错或进行更深度的协作。

